# https://developers.home-assistant.io/docs/add-ons/configuration#add-on-config
name: ollama-gpu-amd
version: 0.3.11-rocm
slug: ollama-gpu-amd
description: "The easiest way to get up and running with large language models."
url: "https://github.com/TheSpaceGod/addon-ollama"
image: "docker.io/ollama/ollama"
arch:
  - amd64
devices:
  - /dev/dri
  - /dev/kfd
init: false
map:
  - config:rw

#Network configuration
ingress: true
ingress_port: 11434
ingress_stream: true
ports:
  11434/tcp: 11434
ports_description:
  11434/tcp: "Ollama API"
watchdog: tcp://[HOST]:[PORT:11434]

#Environmental variables
legacy: true
options:
  OLLAMA_MODELS: "/addons/ollama"
schema:
  OLLAMA_DEBUG: str?
  OLLAMA_FLASH_ATTENTION: str?
  OLLAMA_GPU_OVERHEAD: str?
  OLLAMA_HOST: str?
  OLLAMA_KEEP_ALIVE: str?
  OLLAMA_LLM_LIBRARY: str?
  OLLAMA_LOAD_TIMEOUT: str?
  OLLAMA_MAX_LOADED_MODELS: str?
  OLLAMA_MAX_QUEUE: str?
  OLLAMA_MODELS: str
  OLLAMA_NOHISTORY: str?
  OLLAMA_NOPRUNE: str?
  OLLAMA_NUM_PARALLEL: str?
  OLLAMA_ORIGINS: str?
  OLLAMA_SCHED_SPREAD: str?
  OLLAMA_TMPDIR: str?
  OLLAMA_MULTIUSER_CACHE: str?
  HTTP_PROXY: str?
  HTTPS_PROXY: str?
  NO_PROXY: str?